{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d87b1-d5e7-46a6-b11b-e653cb2477ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import fsspec\n",
    "import gc\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import os\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3bed3-8287-40f2-855e-a6c6faa1831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e19d4-7dd2-4191-8a8d-3d1d239ee412",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_hf = fsspec.filesystem(\"hf\")\n",
    "files_dataset = sorted(entry for entry in fs_hf.glob(\"datasets/Cohere/wikipedia-2023-11-embed-multilingual-v3/fr/*.parquet\"))\n",
    "len(files_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1303db-56c6-4822-8ba8-62cd3f8f0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_cache = Path(\"work\")\n",
    "dir_embeddings = dir_cache / \"embeddings\"\n",
    "dir_embeddings.mkdir(parents=True, exist_ok=True)\n",
    "dir_metadata = dir_cache / \"metadata\"\n",
    "dir_metadata.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16109367-0e11-48a9-a3d4-486120723067",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_ = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941488df-9859-4083-b365-569c66a6c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_index_dataset = dir_cache / \"index.parquet\"\n",
    "if not path_index_dataset.is_file():\n",
    "    while len(titles_) < len(files_dataset):\n",
    "        try:\n",
    "            for path in tqdm(files_dataset, desc=\"Indexing page titles\"):\n",
    "                if path not in titles_:\n",
    "                    with fs_hf.open(path, \"rb\") as file:\n",
    "                        titles_[path] = (\n",
    "                            ParquetFile(file).read(columns=[\"title\"]).to_pandas()\n",
    "                            .groupby(\"title\")\n",
    "                            .agg({\"title\": \"count\"})\n",
    "                            .rename(columns={\"title\": \"num_records\"})\n",
    "                            .reset_index()\n",
    "                            .assign(path=path)\n",
    "                        )\n",
    "        except requests.exceptions.HTTPError:\n",
    "            delay = 90.\n",
    "            print(f\"HuggingFace is protesting our query rate: taking a {delay:.1f}-second pause\")\n",
    "            time.sleep(delay)\n",
    "    pd.concat(titles_.values(), ignore_index=True).to_parquet(path_index_dataset, compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f689f-6b11-405f-8cf2-2f9d055a1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dataset = pd.read_parquet(path_index_dataset)\n",
    "index_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f416369-961e-4d46-94ab-180f37b2c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "size_batch = 10000\n",
    "if \"batch\" not in index_dataset.columns:\n",
    "    index_dataset.sort_values([\"title\", \"path\"], inplace=True)\n",
    "    titles_unique = index_dataset[[\"title\"]].drop_duplicates().sort_index()\n",
    "    batch = []\n",
    "    n = 0\n",
    "    while len(batch) < len(titles_unique):\n",
    "        batch.extend([n] * size_batch)\n",
    "        n += 1\n",
    "    titles_unique[\"batch\"] = batch[:len(titles_unique)]\n",
    "    index_dataset = index_dataset.merge(titles_unique, how=\"inner\", on=\"title\").sort_values([\"batch\", \"title\"]).reset_index(drop=True)\n",
    "    index_dataset.to_parquet(path_index_dataset, compression=\"zstd\")\n",
    "    display(index_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360ad38-1ea9-4559-a854-73239a3ec94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embeddings_fr = dir_cache / \"embeddings-fr.npy\"\n",
    "path_metadata_fr = dir_cache / \"metadata-fr.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcd78b-c926-44f8-bab7-1426f1d230e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_done = {\n",
    "    int(path.with_suffix(\"\").name)\n",
    "    for path in dir_metadata.glob(\"*.parquet\")\n",
    "}\n",
    "index_todo = index_dataset.loc[~index_dataset[\"batch\"].isin(batches_done)]\n",
    "index_todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dae9af-646b-4306-97b9-f2056757bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_todo[\"batch\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d85a7d-6fae-42ef-a9ec-d1fb5b5dd892",
   "metadata": {},
   "source": [
    "Now we need to compute this data for each page. That's a very large groupby operation and follow on calculations. Since we are using dask this is tractable, but it will be very time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8ba65-2ade-4cef-8771-70a4bf7fde41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path):\n",
    "    with fs_hf.open(path, \"rb\") as file:\n",
    "        return pd.read_parquet(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e7db4-fe8c-48bb-bbd2-e7f948c7b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_data_batches(index_todo):\n",
    "    paths_gathered = set()\n",
    "    records = pd.DataFrame(data=[], columns=[\"_id\", \"url\", \"title\", \"text\", \"emb\"])\n",
    "    for i_batch, batch in index_todo.groupby(\"batch\"):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        paths_batch = set(batch[\"path\"])\n",
    "        paths_needed = paths_batch - paths_gathered\n",
    "        if paths_needed:\n",
    "            records = pd.concat([records, *[load_file(path) for path in paths_needed]])\n",
    "        paths_gathered |= paths_batch\n",
    "\n",
    "        titles_batch = batch.groupby(\"title\").agg({\"num_records\": \"sum\"})\n",
    "        records_split = records.merge(titles_batch, how=\"left\", on=\"title\", indicator=True)\n",
    "        records_batch = records_split.loc[records_split[\"_merge\"] == \"both\"].drop(columns=\"_merge\")\n",
    "        assert (\n",
    "            records_batch.groupby(\"title\").agg({\"emb\": \"count\"})\n",
    "            .rename(columns={\"emb\": \"num_records\"})\n",
    "            .sort_values(\"title\")\n",
    "            .equals(\n",
    "                titles_batch.sort_values(\"title\")\n",
    "            )\n",
    "        )\n",
    "        yield i_batch, records_batch\n",
    "\n",
    "        records = records_split.loc[records_split[\"_merge\"] == \"left_only\"].drop(columns=[\"num_records\", \"_merge\"]).copy()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d3073-0692-46f8-aedf-535ff5a82b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def munge_article(article):\n",
    "    embeddings_all = np.vstack(article[\"emb\"])\n",
    "\n",
    "    if len(article) < 2:\n",
    "        topical_sentence = \"\"\n",
    "    else:\n",
    "        embeddings_tail = embeddings_all[1:]\n",
    "        distances = sklearn.metrics.pairwise_distances([np.mean(embeddings_tail, axis=0)], embeddings_tail).squeeze()\n",
    "        closest_idx = np.argmin(distances)\n",
    "        topical_sentence = article[\"text\"].iloc[1:].iloc[closest_idx]\n",
    "\n",
    "    return pd.Series({\n",
    "        \"embeddings\": np.mean(embeddings_all, axis=0),\n",
    "        \"lead_sentences\": article[\"text\"].iloc[0],\n",
    "        \"topical_sentences\": topical_sentence,\n",
    "        \"word_counts\": article[\"text\"].str.split().map(len).sum()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104ddf7-4012-4399-bad8-bd658ff846ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, batch in tqdm(iter_data_batches(index_todo), total=index_todo[\"batch\"].nunique()):\n",
    "    pages = (\n",
    "        batch\n",
    "        .groupby(\"title\")\n",
    "        .apply(munge_article)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"title\": \"titles\"})\n",
    "    )\n",
    "    path_embeddings_batch = dir_embeddings / f\"{i_batch:04d}.npy\"\n",
    "    path_metadata_batch = dir_metadata / f\"{i_batch:04d}.parquet\"\n",
    "    try:\n",
    "        np.save(path_embeddings_batch, np.vstack(pages[\"embeddings\"]))\n",
    "        pages[\n",
    "            [\"titles\", \"lead_sentences\", \"topical_sentences\", \"word_counts\"]\n",
    "        ].to_parquet(path_metadata_batch, compression=\"zstd\")\n",
    "    except:\n",
    "        path_embeddings_batch.unlink(missing_ok=True)\n",
    "        path_metadata_batch.unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15101f-a5d8-4aee-bbf2-00bf7a48b1ec",
   "metadata": {},
   "source": [
    "Since we just did a lot of work, let's be sure to save the results, and set it up so that if we run this again later we can simply load the cached results rather than recomputing things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65bb9f-8ab5-47f3-ba85-3047a977a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all(p.exists() for p in [path_embeddings_fr, path_metadata_fr]):\n",
    "    np.save(\n",
    "        path_embeddings_fr,\n",
    "        np.vstack([\n",
    "            np.load(path)\n",
    "            for path in tqdm(sorted(dir_embeddings.iterdir()), desc=\"Embeddings consolidation\")\n",
    "        ])\n",
    "    )\n",
    "    gc.collect()\n",
    "    pd.concat(\n",
    "        [pd.read_parquet(path) for path in tqdm(sorted(dir_metadata.iterdir()), desc=\"Metadata consolidation\")],\n",
    "        ignore_index=True\n",
    "    ).to_parquet(path_metadata_fr, compression=\"zstd\")\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4108b3-350c-4883-9932-b2c847f86d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectors = np.load(path_embeddings_fr)\n",
    "metadata_fr = pd.read_parquet(path_metadata_fr)\n",
    "metadata_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0720e42f-8415-4c48-9c80-87a33a46e430",
   "metadata": {},
   "source": [
    "## Dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412745ed-0e99-4371-a41a-4bd46cbbfa2d",
   "metadata": {},
   "source": [
    "We are going to need a low dimensional representation for our map. I have my biases in this but let's demonstrate that the tooling that this is a tech-demo for (Toponymy and DataMapPlot) are completely agnostic to how you get your low-dimensional representation and use openTSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f857ea-2054-413b-a669-25dc7fa093c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openTSNE\n",
    "import datashader.mpl_ext\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad3426-2cd1-4b45-987b-a88f92029ce5",
   "metadata": {},
   "source": [
    "As before we'll make sure we save off results so we don't have to redo expensive calculations again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf80e3e-68c8-4c6a-9a5a-b4cb6a54534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path_datamap = dir_cache / \"wikipedia-fr-cohere-embed-datamap.npy\"\n",
    "if not path_datamap.exists():\n",
    "    data_map = openTSNE.TSNE(exaggeration=3., metric=\"cosine\", n_jobs=os.cpu_count(), n_iter=4000, verbose=True).fit(data_vectors)\n",
    "    np.save(path_datamap, data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec5394-1034-432c-860d-49a18de80d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = np.load(path_datamap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f509d-df87-44d6-a119-dcc257ffae4e",
   "metadata": {},
   "source": [
    "And let's have a quick plot to make sure we have something reasonable looking to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3281b5-f03d-45ce-a678-8589d6892b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "datashader.mpl_ext.dsshow(pd.DataFrame({\"x\":data_map[:, 0], \"y\":data_map[:, 1]}), datashader.Point('x', 'y'), aspect=\"equal\", ax=ax, norm=\"eq_hist\",\n",
    "                          cmap=sns.color_palette(\"light:\"+str([matplotlib.colors.rgb2hex(x) for x in sns.color_palette(\"tab10\", 1)][0]), as_cmap=True))\n",
    "_ = ax.set(xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0c903-c0c6-4764-896c-6b242e6f1185",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8510047f-6655-4575-8a6d-9c5926834df2",
   "metadata": {},
   "source": [
    "For topic modelling we are going to use Toponymy. To do that we will need some text content associated to each page. We can create that by gluing together the title, first sentence, and a topical sentence for each page. Let's do that now, ensuring we get everything lined up in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ded47f-a6a9-4e37-8729-6c9721d0478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def shorten(sentence):\n",
    "    return (\n",
    "        sentence[:2045] + \"...\"\n",
    "        if len(sentence) >= 2048\n",
    "        else sentence\n",
    "    )\n",
    "\n",
    "documents = (\n",
    "    metadata_fr[\"titles\"] + \"\\n\\n\"\n",
    "    + metadata_fr[\"lead_sentences\"].progress_map(shorten)\n",
    "    + \"\\n...\\n\"\n",
    "    + metadata_fr[\"topical_sentences\"].progress_map(shorten)\n",
    "    + \"\\n...\\n\"\n",
    ")\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762233b1-6f44-41f8-a309-4d9565d1806e",
   "metadata": {},
   "source": [
    "Now we need to import Toponymy and the associated bits and pieces from it to do this. In particular we are going to need access to an LLM, and an embedding model. Since I had access to Cohere models running on Azure AI Foundry I am going to use those. You will need to make use of whatever services (or local models) you wish. For reference Toponymy supports using any of Cohere, OpenAI, Anthropic, and AzureAI for LLM services, or llama_cpp or Huggingface for running LLMs locally (you'll want a GPU for that). For embedding you can use any SentenceTransformer model (but you'll likely want a GPU) or embedding services from AzureAI, Cohere, and others. Toponymy also has Async versions of most of the llm_wrappers which batch queries adn can run a lot faster (depending on your service rate limits). Those had not been fully implemented when I first ran this, so I will stick with the approach I used at the time. If you want to speed things up I recommend exploring the Async llm_warppers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eaf7e3-ab9f-4ea0-a9ce-8b31adb5436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toponymy import Toponymy, ToponymyClusterer, KeyphraseBuilder, ClusterLayerText\n",
    "from toponymy.llm_wrappers import AsyncAzureAI, AzureAI\n",
    "from toponymy.embedding_wrappers import AzureAIEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a584847-7848-4d6f-b4f4-29ff00d039ce",
   "metadata": {},
   "source": [
    "Obviously you'll need to load your own API keys here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04bd48e-0fb7-4698-80ad-0e87bf8ace64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "from getpass import getpass\n",
    "try:\n",
    "    azure_api_key = os.environ[\"AZURE_COHERE_API_KEY\"]\n",
    "except KeyError:\n",
    "    azure_api_key = getpass(\"Copy-paste your API key here:\")\n",
    "    Path(\".env\").write_text(f'AZURE_COHERE_API_KEY = \"{azure_api_key}\"\\n', encoding=\"utf-8\")\n",
    "    print(\"It's saved to local file .env for future runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcd2de-00f7-4945-8241-e0dfb43c7377",
   "metadata": {},
   "source": [
    "Now we'll instantiate the llm, embedding model, clusterer and keyphrase builder that Toponymy will use. You will have to swap in your preferred services or local models here, but the clusterer and keyphrase builder should be fine as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c42fa5-3eee-4057-96e1-ca00ab4f0f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AsyncAzureAI(\n",
    "    api_key=azure_api_key,\n",
    "    endpoint=\"https://azureaitimcuse5821437469.services.ai.azure.com/models\",\n",
    "    model=\"Cohere-command-r-08-2024\",\n",
    "    llm_specific_instructions=\"Topic names should be in français not English\",\n",
    ")\n",
    "embedding_model = AzureAIEmbedder(api_key=azure_api_key, endpoint=\"https://azureaitimcuse5821437469.services.ai.azure.com/models\", model=\"Cohere-embed-v3-multilingual\")\n",
    "clusterer = ToponymyClusterer(min_clusters=8, base_n_clusters=16000, verbose=True)\n",
    "keyphrase_builder = KeyphraseBuilder(verbose=True, n_jobs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19423bf-b30b-48f8-a205-0f6e4e6d7681",
   "metadata": {},
   "source": [
    "We can now create a topic modeller as a Toponymy object. It helps to provide a description of the objects (documents) and corpus as this will aid in providing more accurate names. In our case that's straightforward since we are working with Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9802bc7-9230-4eb4-9e9e-0fa8444b3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_namer = Toponymy(\n",
    "    llm,\n",
    "    embedding_model,\n",
    "    clusterer=clusterer,\n",
    "    keyphrase_builder=keyphrase_builder,\n",
    "    object_description='Wikipedia article title and excerpt',\n",
    "    corpus_description='English language Wikipedia',\n",
    "    show_progress_bars=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4c9cf-21e6-441e-b586-20a3f937684e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Running the next cell will start the topic naming process. This will take a long time, and more importantly, if you are using an LLM service cost you <b>actual money</b>. I have run a similar workload to this using Claude-Haiku-3, and it cost around $15, but a lot will depend on the service and model you select. As a general rule the cheapest LLMs available from Anthropic, OpenAI, and Cohere are generally powerful enough to do a good job, and you get significantly diminishing returns for more expensive models.</div>\n",
    "\n",
    "As usual we are going to save off results here so we don't have to rerun this all later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d8f6b6-bdcf-4f45-a931-30bbf6ebf7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if all((dir_cache / f\"wikipedia_fr_azureai_2025-05_topic_layer_{i}.npy\").is_file() for i in range(7)):\n",
    "    print(\"Reading results from disk...\")\n",
    "    topic_layers = []\n",
    "    for i in range(7):\n",
    "        topic_layers.append(np.load(dir_cache / f\"wikipedia_fr_azureai_2025-05_topic_layer_{i}.npy\", allow_pickle=True))\n",
    "else:\n",
    "    topic_namer.fit(documents, data_vectors, data_map, exemplar_method=\"central\")\n",
    "\n",
    "    for i, layer in enumerate(topic_namer.topic_name_vectors_):\n",
    "        np.save(dir_cache / f\"wikipedia_fr_azureai_2025-05_topic_layer_{i}.npy\", layer)\n",
    "    topic_layers = topic_namer.topic_name_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0fefa-637d-47fb-8d04-92a011e5af07",
   "metadata": {},
   "source": [
    "**Note:** the async LLM wrapper hung up at some point. It seems like the coroutine either never finished, or left stuff hanging that made `run_until_complete` hang."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0891d17-34e0-4ecc-b36e-179545923969",
   "metadata": {},
   "source": [
    "## Visualizing the Data Map\n",
    "\n",
    "This is the real goal here -- making useful visualizations of the entire content of English Wikipedia. For that we are going to use datamapplot. Since we'll be building an interactive version in due course we'll also need to import the selection_handlers module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ab7fc-0bec-4dc7-b65a-280468d25de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamapplot\n",
    "import datamapplot.selection_handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a10563-faa0-421e-a6fe-a40503e23bfd",
   "metadata": {},
   "source": [
    "Since Cohere did a lot of the heavy lifting for us (providing embeddings of all the text on Wikipedia and releasing that as an accessible dataset, as well as helping out with topic modelling) let's grab the Cohere logo that we can stick on our plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e2947-b573-4f6c-b404-2fc459b25588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import PIL\n",
    "cohere_logo_response = requests.get(\n",
    "    \"https://asset.brandfetch.io/idfDTLvPCK/idyv4d98RT.png\",\n",
    "    stream=True,\n",
    ")\n",
    "cohere_logo = np.asarray(PIL.Image.open(cohere_logo_response.raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d69e67-09f7-4e38-8a54-97edd672cc0d",
   "metadata": {},
   "source": [
    "Before embarking on an interactive plot, let's make a static plot so we can check that we have something reasonable here. We'll use the topic layer 5 for topic labels, since this is one layer down from the uppermost labels (which will likely be few and fairly high level), but not so far down as to be in the weeds of Wikipedia with far too many far too detailed topics. Beyond that it is a matter of adding some aesthetic styling with DataMapPlot -- we'll make the topic indicator arrows a little more stylish, Choose a nice font, and pick out a more interesting background colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a0e5e-e022-4bc4-996a-ec3eabae0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = datamapplot.create_plot(\n",
    "    data_map.astype(np.float32),\n",
    "    topic_layers[5],\n",
    "    title=\"Carte de Wikipedia\",\n",
    "    sub_title=\"Les pages de Wikipedia en français vectorisées avec Cohere Embed\",\n",
    "    logo=cohere_logo,\n",
    "    logo_width=0.28,\n",
    "    use_medoids=True,\n",
    "    arrowprops={\"arrowstyle\": \"wedge,tail_width=0.85,shrink_factor=0.15\", \"linewidth\": 0.4, \"fc\": \"#33333377\", \"ec\": \"#333333aa\"},\n",
    "    font_family=\"Marcellus SC\",\n",
    "    label_linespacing=1.25,\n",
    "    label_direction_bias=1.25,\n",
    "    title_keywords={\"fontsize\":61.5},\n",
    "    figsize=(15,15)\n",
    ")\n",
    "_ = ax.set(facecolor=\"#eae6de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6266e4-4e9e-41d5-aad8-a38fd4b8ac6d",
   "metadata": {},
   "source": [
    "Now we want to make an interactive plot. We can do that as simply as calling ``datamapplot.create_interactive_plot``, but we would like to have more functionality available than the most basic versions will provide. To start with it would be good if hovering over points could actually provide useful information about the page. At the same time, we don't want to have to load the entire content of Wikipedia. We can make do with a compromise -- if we have load in only the titles of all the pages, we can use the Wikipedia API to get extracts for the page on an as-needed basis. To enable this within DataMapPlot we will have to provide (javascript -- because this all has to live client-side) code for how to fetch the data given a page title, how to format the results we'll get back into an HTML tooltip, and what to display in the meantime. Let's define all of that in advance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a76af61-709b-4546-93f7-905a6a1f4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_tooltip = {\n",
    "    \"fetch_js\": \"\"\"\n",
    "async (identifier) => {\n",
    "    const apiUrl = `https://fr.wikipedia.org/w/api.php?` +\n",
    "      `action=query&` +\n",
    "      `format=json&` +\n",
    "      `prop=extracts|pageimages&` +\n",
    "      `exintro=1&` +\n",
    "      `explaintext=1&` +\n",
    "      `pithumbsize=300&` +\n",
    "      `titles=${encodeURIComponent(identifier)}&` +\n",
    "      `origin=*`;\n",
    "\n",
    "    const response = await fetch(apiUrl);\n",
    "    const json = await response.json();\n",
    "    const page = Object.values(json.query.pages)[0];\n",
    "\n",
    "    const content = {\n",
    "      title: page.title,\n",
    "      extract: page.extract,\n",
    "      thumbnail: page.thumbnail?.source,\n",
    "      url: `https://fr.wikipedia.org/wiki/${encodeURIComponent(page.title)}`\n",
    "    };\n",
    "    return content;\n",
    "}\n",
    "    \"\"\",\n",
    "    \"format_js\": \"\"\"\n",
    "(data) => {\n",
    "    return `\n",
    "      <h3 style=\"margin: 0 0 8px 0\">${data.title}</h3>\n",
    "      ${data.thumbnail ?\n",
    "        `<img src=\"${data.thumbnail}\"\n",
    "          style=\"float: right; max-width: 100px; margin: 0 0 8px 8px;\"\n",
    "          alt=\"${data.title}\"/>`\n",
    "        : ''}\n",
    "      <p style=\"margin: 0; line-height: 1.4;\">\n",
    "        ${data.extract.split('. ').slice(0, 3).join('. ')}.\n",
    "      </p>\n",
    "    `;\n",
    "}\n",
    "    \"\"\",\n",
    "    \"loading_js\": \"\"\"\n",
    "(identifier) => {\n",
    "    return `\n",
    "      <h3 style=\"margin: 0 0 8px 0\">${identifier}</h3>\n",
    "      Chargement ...\n",
    "      `\n",
    "}\n",
    "    \"\"\",\n",
    "    \"error_js\": \"\"\"\n",
    "(error, identifier) => `Erreur durant le chargement de l'information sur ${identifier}: ${error.message}`\n",
    "    \"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03791f7f-0e95-4106-85fe-b4bcdaf8983a",
   "metadata": {},
   "source": [
    "It can also be useful to have word-clouds of the text of selected items appear using lasso-style selections. There is already tooling in DataMapPlot to do this. Unfortunately that tooling is built mostly for smaller datasets than the 5+ million pages of English Wikipedia and assumes that we have all the text content we want to build word-clouds from in memory on the client side (i.e. the text we would be using for tooltips). Since we are fetching text content on-demand for tooltips we will have to do the same for the word-clouds. That means we need a modified ``WordCloud`` selection handler. This is actually a relatively small change to the existing ``WordCloud`` selection handler, so in practice we will grab that code and make the minor modifications required to have it fetch text data rather than assuming it is already in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005b6f1-fac2-4405-a8ce-64b6ef964777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamapplot.selection_handlers import SelectionHandlerBase\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import string\n",
    "\n",
    "from datamapplot.config import ConfigManager\n",
    "cfg = ConfigManager()\n",
    "\n",
    "class WikipediaWordCloud(SelectionHandlerBase):\n",
    "    \"\"\"A selection handler that generates a word cloud from the selected text items by pulling the\n",
    "    relevant text from Wikipedia pages with the titles given by the selection. The word cloud\n",
    "    is displayed in a container on the page, and the number of words in the cloud can be controlled\n",
    "    by the `n_words` parameter.\n",
    "\n",
    "    The word cloud is generated using the d3-cloud library, and the appearance of the word cloud can\n",
    "    be customized using the `width`, `height`, `font_family`, `stop_words`, `n_rotations`, and `color_scale`\n",
    "    parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_words : int, optional\n",
    "        The number of words to display in the word cloud. Default is 256.\n",
    "\n",
    "    width : int, optional\n",
    "        The width of the word cloud container. Default is 500.\n",
    "\n",
    "    height : int, optional\n",
    "        The height of the word cloud container. Default is 500.\n",
    "\n",
    "    font_family : str, optional\n",
    "        The font family to use for the word cloud. Default is None.\n",
    "\n",
    "    stop_words : list, optional\n",
    "        A list of stop words to exclude from the word cloud. Default is the English stop words from scikit-learn.\n",
    "\n",
    "    n_rotations : int, optional\n",
    "        The number of rotations to use for the words in the word cloud. Default is 0. More rotations can make the\n",
    "        word cloud more visually interesting, at the cost of readability.\n",
    "\n",
    "    color_scale : str, optional\n",
    "        The color scale to use for the word cloud. Default is \"YlGnBu\". The color scale can be any d3 color scale\n",
    "        name, with an optional \"_r\" suffix to reverse the color scale.\n",
    "\n",
    "    location : str, optional\n",
    "        The location of the word cloud container on the page. Default is \"bottom-right\".\n",
    "        Should be one of \"top-left\", \"top-right\", \"bottom-left\", or \"bottom-right\".\n",
    "\n",
    "    **kwargs\n",
    "        Additional keyword arguments to pass to the SelectionHandlerBase constructor.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @cfg.complete(unconfigurable={\"self\", \"width\", \"height\", \"n_words\"})\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_words=256,\n",
    "        width=500,\n",
    "        height=500,\n",
    "        font_family=None,\n",
    "        stop_words=None,\n",
    "        n_rotations=0,\n",
    "        use_idf=False,\n",
    "        color_scale=\"YlGnBu\",\n",
    "        location=\"bottom-right\",\n",
    "        cdn_url=\"unpkg.com\",\n",
    "        other_triggers=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dependencies=[\n",
    "                f\"https://{cdn_url}/d3@latest/dist/d3.min.js\",\n",
    "                f\"https://{cdn_url}/d3-cloud@1.2.7/build/d3.layout.cloud.js\",\n",
    "                f\"https://{cdn_url}/jquery@3.7.1/dist/jquery.min.js\",\n",
    "            ],\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.n_words = n_words\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.font_family = font_family\n",
    "        self.stop_words = stop_words or list(ENGLISH_STOP_WORDS)\n",
    "        self.n_rotations = min(22, n_rotations)\n",
    "        self.use_idf = str(use_idf).lower()\n",
    "        self.location = location\n",
    "        if color_scale.endswith(\"_r\"):\n",
    "            self.color_scale = string.capwords(color_scale[:1]) + color_scale[1:-2]\n",
    "            self.color_scale_reversed = True\n",
    "        else:\n",
    "            self.color_scale = string.capwords(color_scale[:1]) + color_scale[1:]\n",
    "            self.color_scale_reversed = False\n",
    "        self.other_triggers = other_triggers\n",
    "\n",
    "    @property\n",
    "    def javascript(self):\n",
    "        result = f\"\"\"\n",
    "const _STOPWORDS = new Set({self.stop_words});\n",
    "const _ROTATIONS = [0, -90, 90, -45, 45, -30, 30, -60, 60, -15, 15, -75, 75, -7.5, 7.5, -22.5, 22.5, -52.5, 52.5, -37.5, 37.5, -67.5, 67.5];\n",
    "let wordCloudStackContainer = document.getElementsByClassName(\"stack {self.location}\")[0];\n",
    "const wordCloudItem = document.createElement(\"div\");\n",
    "wordCloudItem.id = \"word-cloud\";\n",
    "wordCloudItem.className = \"container-box more-opaque stack-box\";\n",
    "wordCloudStackContainer.appendChild(wordCloudItem);\n",
    "\n",
    "const wordCloudSvg = d3.select(\"#word-cloud\").append(\"svg\")\n",
    "    .attr(\"width\", {self.width})\n",
    "    .attr(\"height\", {self.height})\n",
    "    .append(\"g\")\n",
    "    .attr(\"transform\", \"translate(\" + {self.width} / 2 + \",\" + {self.height} / 2 + \")\");\n",
    "\n",
    "var wordCounter = null;\n",
    "wordCounter = function (textItems) {{\n",
    "    const words = textItems.join(' ').toLowerCase().split(/\\\\s+/);\n",
    "    const wordCounts = new Map();\n",
    "    words.forEach(word => {{\n",
    "        wordCounts.set(word, (wordCounts.get(word) || 0) + 1);\n",
    "    }});\n",
    "    _STOPWORDS.forEach(stopword => wordCounts.delete(stopword));\n",
    "    const result = Array.from(wordCounts, ([word, frequency]) => ({{ text: word, size: Math.sqrt(frequency) }}))\n",
    "                        .sort((a, b) => b.size - a.size).slice(0, {self.n_words});\n",
    "    const maxSize = Math.max(...(result.map(x => x.size)));\n",
    "    return result.map(({{text, size}}) => ({{ text: text, size: (size / maxSize)}}));\n",
    "}}\n",
    "\n",
    "// Cache for API responses\n",
    "const batchCache = new Map();\n",
    "\n",
    "async function fetchWikiContentBatch(titles, maxTitles = 250) {{\n",
    "    // Deduplicate and filter already cached titles\n",
    "    const uniqueTitles = [...new Set(titles)];\n",
    "    const titlesToFetch = uniqueTitles.filter(title => !batchCache.has(title));\n",
    "\n",
    "    // Split titles into chunks of 50 (API limit per request)\n",
    "    const chunks = [];\n",
    "    for (let i = 0; i < Math.min(titlesToFetch.length, maxTitles); i += 50) {{\n",
    "        chunks.push(titlesToFetch.slice(i, i + 50));\n",
    "    }}\n",
    "\n",
    "    // Process each chunk\n",
    "    const results = [];\n",
    "\n",
    "    try {{\n",
    "        // Fetch all chunks in parallel\n",
    "        await Promise.all(chunks.map(async (titleChunk) => {{\n",
    "            const titleString = titleChunk.join('|');\n",
    "            const apiUrl = `https://fr.wikipedia.org/w/api.php?` +\n",
    "                `action=query&` +\n",
    "                `format=json&` +\n",
    "                `prop=extracts&` +\n",
    "                `exintro=0&` + // Get full content, not just intro\n",
    "                `explaintext=1&` +\n",
    "                `titles=${{encodeURIComponent(titleString)}}&` +\n",
    "                `origin=*`;\n",
    "\n",
    "            const response = await fetch(apiUrl);\n",
    "            const json = await response.json();\n",
    "\n",
    "            // Process each page in the response\n",
    "            Object.values(json.query.pages).forEach(page => {{\n",
    "                // Store in both results and cache\n",
    "                if (page.extract) {{\n",
    "                  results.push(page.extract);\n",
    "                  batchCache.set(page.title, page.extract);\n",
    "                }} else {{\n",
    "                  results.push(page.title);\n",
    "                }}\n",
    "            }});\n",
    "        }}));\n",
    "\n",
    "        // Combine cached and new results\n",
    "        return results.concat(Array.from(uniqueTitles).filter(title => batchCache.has(title)).map(title => batchCache.get(title)));\n",
    "\n",
    "    }} catch (error) {{\n",
    "        console.error('Error fetching Wikipedia content batch:', error);\n",
    "        return new Map();\n",
    "    }}\n",
    "}}\n",
    "\n",
    "function generateWordCloud(words) {{\n",
    "  const width = {self.width};\n",
    "  const height = {self.height};\n",
    "\n",
    "  const colorScale = d3.scaleSequential(d3.interpolate{self.color_scale}).domain([{\"width / 10, 0\" if self.color_scale_reversed else \"0, width / 10\"}]);\n",
    "\n",
    "  // Configure a cloud layout\n",
    "  const layout = d3.layout.cloud()\n",
    "    .size([width, height])\n",
    "    .words(words.map(d => ({{text: d.text, size: d.size * width / 10}})))\n",
    "    .padding(1)\n",
    "    .rotate(() => _ROTATIONS[~~(Math.random() * {self.n_rotations})])\n",
    "    .font(\"{self.font_family or 'Impact'}\")\n",
    "    .fontSize(d => d.size)\n",
    "    .fontWeight(d => Math.max(300, Math.min(d.size * 9000 / width, 900)))\n",
    "    .on(\"end\", draw);\n",
    "\n",
    "  layout.start();\n",
    "\n",
    "  function draw(words) {{\n",
    "    const t = d3.transition().duration(300);\n",
    "\n",
    "    // Update existing words\n",
    "    const text = wordCloudSvg.selectAll(\"text\")\n",
    "      .data(words, d => d.text);\n",
    "\n",
    "    // Remove old words\n",
    "    text.exit()\n",
    "      .transition(t)\n",
    "      .attr(\"fill-opacity\", 0)\n",
    "      .attr(\"font-size\", 1)\n",
    "      .remove();\n",
    "    // Add new words\n",
    "    text.enter()\n",
    "      .append(\"text\")\n",
    "      .attr(\"text-anchor\", \"middle\")\n",
    "      .attr(\"fill-opacity\", 0)\n",
    "      .attr(\"font-size\", 1)\n",
    "      .attr(\"font-family\", \"{self.font_family or 'Impact'}\")\n",
    "      .text(d => d.text)\n",
    "      .merge(text) // Merge enter and update selections\n",
    "      .transition(t)\n",
    "      .attr(\"transform\", d => \"translate(\" + [d.x, d.y] + \")rotate(\" + d.rotate + \")\")\n",
    "      .attr(\"fill-opacity\", 1)\n",
    "      .attr(\"font-size\", d => d.size)\n",
    "      .attr(\"font-weight\", d => Math.max(300, Math.min(d.size * 9000 / width, 900)))\n",
    "      .attr(\"fill\", d => colorScale(d.size));\n",
    "  }}\n",
    "}}\n",
    "\n",
    "const shuffle = ([...arr]) => {{\n",
    "  let m = arr.length;\n",
    "  while (m) {{\n",
    "    const i = Math.floor(Math.random() * m--);\n",
    "    [arr[m], arr[i]] = [arr[i], arr[m]];\n",
    "  }}\n",
    "  return arr;\n",
    "}};\n",
    "const sampleSize = ([...arr], n = 1) => shuffle(arr).slice(0, n);\n",
    "\n",
    "async function wordCloudCallback(selectedPoints) {{\n",
    "    if (selectedPoints.length > 0) {{\n",
    "        $(wordCloudItem).animate({{height:'show'}}, 250);\n",
    "    }} else {{\n",
    "        $(wordCloudItem).animate({{height:'hide'}}, 250);\n",
    "    }}\n",
    "    let selectedText;\n",
    "    if (datamap.metaData) {{\n",
    "        selectedText = await fetchWikiContentBatch(sampleSize(selectedPoints, 500).map(i => datamap.metaData.hover_text[i]));\n",
    "    }} else {{\n",
    "        selectedText = [\"Le chargement des métadonnées est en cours...\"];\n",
    "    }}\n",
    "    const wordCounts = wordCounter(selectedText);\n",
    "    generateWordCloud(wordCounts);\n",
    "}}\n",
    "\n",
    "await datamap.addSelectionHandler(debounce(wordCloudCallback));\n",
    "\"\"\"\n",
    "        if self.other_triggers:\n",
    "            for trigger in self.other_triggers:\n",
    "                result += f\"\"\"await datamap.addSelectionHandler(debounce(wordCloudCallback), \"{trigger}\");\\n\"\"\"\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def html(self):\n",
    "        # return \"\"\"<div id=\"word-cloud\" class=\"container-box more-opaque\"></div>\"\"\"\n",
    "        return \"\"\n",
    "\n",
    "    @property\n",
    "    def css(self):\n",
    "        return f\"\"\"\n",
    "#word-cloud {{\n",
    "    position: relative;\n",
    "    display: none;\n",
    "    width: {self.width}px;\n",
    "    height: {self.height}px;\n",
    "    z-index: 10;\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9b55dc-96d0-4d7e-b79b-8152dbee6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "STOP_WORDS_FR = get_stop_words(\"fr\")\n",
    "pd.Series(STOP_WORDS_FR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd2340-c5b0-4342-a506-0739e358fefd",
   "metadata": {},
   "source": [
    "It will be good to have an info-box item that a user can click on to have it expand and provide more information about the page. This isn't available by default in DataMapPlot, but we can provide our own custom CSS and javascript, so let's create what we need to allow for a simple clickable info box that has the text content we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df409112-a8cb-49f6-8f04-955985339e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_css = \"\"\"\n",
    "#info-box {\n",
    "    width: fit-content;\n",
    "    height: fit-content;\n",
    "    z-index: 10;\n",
    "    overflow: hidden;\n",
    "    width: 20px;\n",
    "    height: 20px;\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    justify-content: center;\n",
    "}\n",
    "\n",
    "#info-icon {\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    justify-content: center;\n",
    "    width: 100%;\n",
    "    height: 100%;\n",
    "}\n",
    "\n",
    "#info-icon svg {\n",
    "    width: 16px;\n",
    "    height: 16px;\n",
    "}\n",
    "\n",
    "#info-content {\n",
    "    display: none;\n",
    "    position: absolute;\n",
    "    padding: 12px;\n",
    "    box-sizing: border-box;\n",
    "    width: 100%;\n",
    "    height: 100%;\n",
    "    overflow-y: auto;\n",
    "    pointer-events: none;\n",
    "}\n",
    "\n",
    "#info-content h3 {\n",
    "    margin-top: 0;\n",
    "    margin-bottom: 10px;\n",
    "    color: #0056b3;\n",
    "    font-size: 1.1em;\n",
    "}\n",
    "\n",
    "#info-content p {\n",
    "    margin-bottom: 0;\n",
    "    font-size: 0.9em;\n",
    "    line-height: 1.2;\n",
    "}\n",
    "\n",
    "#info-box.expanded {\n",
    "    width: 600px;\n",
    "    height: auto;\n",
    "    min-height: 100px;\n",
    "    max-height: 800px;\n",
    "    border: 1px solid #ccc;\n",
    "    align-items: flex-start;\n",
    "    justify-content: flex-start;\n",
    "    overflow-y: auto;\n",
    "}\n",
    "\n",
    "#info-box.expanded #info-icon {\n",
    "    pointer-events: none;\n",
    "    display: none;\n",
    "}\n",
    "\n",
    "#info-box.expanded #info-content {\n",
    "    position: relative;\n",
    "    display: block;\n",
    "    pointer-events: auto;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "custom_js = \"\"\"\n",
    "const infoWidget = document.createElement(\"div\");\n",
    "infoWidget.id = \"info-box\";\n",
    "infoWidget.className = \"container-box more-opaque stack-box\";\n",
    "infoWidget.innerHTML = `\n",
    "<div id=\"info-icon\">\n",
    "  <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" fill=\"currentColor\">\n",
    "    <path d=\"M8 15A7 7 0 1 1 8 1a7 7 0 0 1 0 14zm0 1A8 8 0 1 0 8 0a8 8 0 0 0 0 16z\"/>\n",
    "    <path d=\"m8.93 6.588-2.29.287-.082.38.45.083c.294.07.352.176.288.469l-.738 3.468c-.194.897.105 1.319.808 1.319.545 0 1.178-.252 1.465-.598l.088-.416c-.2.176-.492.246-.686.246-.275 0-.375-.193-.304-.533L8.93 6.588zM9 4.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0z\"/>\n",
    "  </svg>\n",
    "</div>\n",
    "<div id=\"info-content\">\n",
    "    <h3>À propos de cette <em>carte de données</em></h3>\n",
    "    <p>\n",
    "        Cette page constitue une carte explorable de Wikipedia en français.\n",
    "        Elle a été constuite en utilisant des\n",
    "        <a href=\"https://huggingface.co/datasets/Cohere/wikipedia-2023-11-embed-multilingual-v3\">plongements\n",
    "        compilés par Cohere</a>, openTSNE, <a href=\"https://github.com/TutteInstitute/toponymy\">Toponymy</a>,\n",
    "        ainsi que <a href=\"https://github.com/TutteInstitute/datamapplot\">DataMapPlot</a>.\n",
    "        Elle existe en tant que vitrine pour certaines des fonctionnalités\n",
    "        des librairies DataMapPlot et Toponymy.\n",
    "    </p>\n",
    "    <p>\n",
    "        Les étiquettes des sujets ont été générées automatiquement en utilisant\n",
    "        Toponymy et le LLM Cohere Command-R.\n",
    "    </p>\n",
    "    <p>\n",
    "        Cliquez et glissez pour déplacer la carte, utilisez la roulette de souris ou pincez l'écran pour zoomer.\n",
    "        Cliquer sur un point naviguera à la page correspondante.<br/>\n",
    "        L'arbre des sujets (<em>topic tree</em>) facilite le déplacement vers des sujets spécifiques.\n",
    "        Développez les sujets en cliquant sur le triangle pour révéler des sous-sujets plus précis.\n",
    "        Cliquez sur le nom d'un sujet pour centrer la vue de la carte sur sa position.<br/>\n",
    "        En tenant Shift, cliquez et glissez pour sélectionner une région:\n",
    "        cela générera un <em>nuage de mots</em> à partir du contenu des pages sélectionnées.\n",
    "        Sélectionnez une région vide pour faire disparaître l'encart du nuage.\n",
    "    </p>\n",
    "</div>\n",
    "`\n",
    "const stackContainer = document.getElementsByClassName(\"stack bottom-left\")[0];\n",
    "stackContainer.insertBefore(infoWidget, stackContainer.firstChild);\n",
    "if (infoWidget) {\n",
    "    infoWidget.addEventListener('click', function() {\n",
    "        infoWidget.classList.toggle('expanded');\n",
    "    });\n",
    "} else {\n",
    "    console.error(\"On ne trouve pas l'élément avec ID 'info-widget'.\");\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5d911-2e28-4eeb-855c-0ac3e003d9b3",
   "metadata": {},
   "source": [
    "Last of all, while we have the word-count of the page, it would be nice to have some other metadata about the pages that we can display (potentially looking at other colourmaps over the data for example). I collected this data for the pages listed in the 2023 pull from Cohere -- counts of incoming links, outgoing links, number of categories the page belongs to, and the number of page-views in the last month. This is simply a matter of using the Wikipedia REST API to query for information; to do this and follow rate-limits etc. is more complicated than worth showing here, and can be quite time-consuming (based on the rate limits and the number of pages we need to query data for). I did all the querying and then saved the dataset and uploaded it to Huggingface datasets for others to use. We can load that data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd2d7d-01e3-4d07-a878-4eec4fac8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_page_stats = pd.read_parquet(\"hf://datasets/lmcinnes/wikipedia_page_statistics/wikipedia_page_stats.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac88e1-a35a-4caa-b92e-c67aae35ffa1",
   "metadata": {},
   "source": [
    "Now it is time to make our interactive plot. We will need to provide the data map, all the topic name layers, hover text which is the name of page, and some styling options. To make things a little more interesting we can add search functionality (to search by page title), an on-click action that will take you to the actual Wikipedia page, a selection handler that will pop up a word-cloud based on the contents of the pages selected, the ability to colour points by incoming links, number of categories, or page views, a \"table-of-contents\" that allows navigation by a tree of topics generated by Toponymy, a splash warning, and our custom tooltip, and info-box. There's quite a lot of data here, and a bunch of work to compile it, so this will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4f38c-ca81-42d9-a658-a4b0dc1c8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wikipedia_plot_bundle = datamapplot.create_interactive_plot(\n",
    "    data_map,\n",
    "    *topic_layers,\n",
    "    hover_text=metadata_fr.titles.values,\n",
    "    cluster_boundary_polygons=True,\n",
    "    polygon_alpha=0.1,\n",
    "    cluster_boundary_line_width=0.2,\n",
    "    enable_search=True,\n",
    "    point_radius_min_pixels=0.2,\n",
    "    point_radius_max_pixels=16,\n",
    "    point_line_width=0,\n",
    "    marker_size_array=np.sqrt(metadata_fr.word_counts.values),\n",
    "    initial_zoom_fraction=0.9,\n",
    "    font_family=\"Cinzel\",\n",
    "    title=\"Carte interactive de Wikipedia\",\n",
    "    sub_title=\"Une carte des articles en français de Wikipedia. Les articles similaires correspondent à des points rapprochés.\",\n",
    "    logo=\"https://asset.brandfetch.io/idfDTLvPCK/idyv4d98RT.png\",\n",
    "    on_click=\"window.open(`https://fr.wikipedia.org/wiki/${{encodeURIComponent(hoverData.hover_text[index])}}`)\",\n",
    "    selection_handler=WikipediaWordCloud(512, width=640, height=480, font_family=\"Cinzel\", stop_words=STOP_WORDS_FR, n_rotations=1, location=\"top-right\"),\n",
    "    colormap_rawdata=[\n",
    "        np.log10(1 + wikipedia_page_stats.incoming_link_count),\n",
    "        np.log10(1 + wikipedia_page_stats.category_count),\n",
    "        np.log10(1 + wikipedia_page_stats.page_view_count)\n",
    "    ],\n",
    "    colormap_metadata=[\n",
    "        {\"field\": \"inlinks\", \"description\": \"Log du nombre de liens antécédents\", \"cmap\": \"mako_r\", \"kind\": \"continuous\"},\n",
    "        {\"field\": \"categories\", \"description\": \"Log du nombre de catégories\", \"cmap\": \"rocket_r\", \"kind\": \"continuous\"},\n",
    "        {\"field\": \"views\", \"description\": \"Log du nombre de visites par mois\", \"cmap\": \"YlGn\", \"kind\": \"continuous\"},\n",
    "    ],\n",
    "    enable_topic_tree=True,\n",
    "    topic_tree_kwds={\"color_bullets\": True},\n",
    "    splash_warning=\"<p>Cette page va télécharger environ 200MB de données pour construire une carte interactive de Wikipedia.</p><p>Ce chargement initial peut prendre un bon moment...</p>\",\n",
    "    inline_data=False,\n",
    "    offline_data_chunk_size=400_000,\n",
    "    dynamic_tooltip=api_tooltip,\n",
    "    custom_js=custom_js,\n",
    "    custom_css=custom_css,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c0942-82e5-4f69-afc1-a029cc90c44a",
   "metadata": {},
   "source": [
    "With that done all we have to do is save the results. Since this is a lot of data the files are chunked up, and our best approach is to use the ``save_bundle`` method that will put all the relevant material into a single zip file. You can then unzip that file on a webserver (or unzip it locally and run a local server with ``python -m http.server -b 127.0.0.1 8080`` or similar) and be good to go. If you are not worried about having a hideously large single HTML file that you can just view locally, simply comment out the lines\n",
    "\n",
    "```python\n",
    "    inline_data=False,\n",
    "    offline_data_chunk_size=400_000,\n",
    "```\n",
    "\n",
    "from the above cell, rerun, and then use\n",
    "\n",
    "```python\n",
    "wikipedia_plot_bundle.save('wikipedia_data_map.html')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad421c1-3d80-4a14-a640-9fba404565fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_plot_bundle.save_bundle('wikipedia-fr_data_map.zip')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0d711d7-4e18-4da9-8fb9-4abb5a594fc4",
   "metadata": {},
   "source": [
    "wikipedia_plot_bundle.save('wikipedia-fr_data_map.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMapPlot Examples",
   "language": "python",
   "name": "datamapplot_examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
